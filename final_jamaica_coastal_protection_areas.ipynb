{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon, MultiPoint, MultiPolygon, LineString\n",
    "from shapely.ops import unary_union, voronoi_diagram, nearest_points\n",
    "from shapely import affinity\n",
    "\n",
    "from scipy.ndimage import label\n",
    "from scipy.spatial import Voronoi\n",
    "from scipy.optimize import minimize\n",
    "from skimage import measure\n",
    "\n",
    "import rasterio\n",
    "from rasterio.transform import from_origin\n",
    "from rasterstats import zonal_stats\n",
    "\n",
    "import sklearn.cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raster_file = \"data/flood_rasters/JamaicaJAM001RCP852100_epsg_32618_RP_100.tif\"  #raster file containing flood data, used for analysis or overlays\n",
    "inland_buffer_distance = 2  # Buffer distance (in kilometers) to expand the inital bounding boxes\n",
    "depth_thresh = 0\n",
    "eps_thresh = 3\n",
    "return_period = 100\n",
    "\n",
    "\n",
    "#Not defined by user\n",
    "input_file = \"data/Foundation_Data.gpkg\"  # GeoPackage file with coastline edges and nodes\n",
    "processing_file = \"outputs/coastal_protection_processing_layers.gpkg\"  # GeoPackage file to store intermediate processing layers\n",
    "output_file = \"outputs/Jamaica_coastal_protection_areas.gpkg\"  # Final output file for coastal protection areas\n",
    "\n",
    "coastline_edge_layer = gpd.read_file(input_file, layer=\"edges\")  # Load edges as GeoDataFrame\n",
    "coastline_node_layer = gpd.read_file(input_file, layer=\"nodes\")  # Load nodes as GeoDataFrame\n",
    "\n",
    "# coastline_edge_layer.to_file(output_file, layer=\"edges\", driver=\"GPKG\")  # Save edges to output file\n",
    "# coastline_node_layer.to_file(output_file, layer=\"nodes\", driver=\"GPKG\")  # Save nodes to output file\n",
    "\n",
    "jamaica_polygon_revised = gpd.read_file(input_file, layer=\"jam\") \n",
    "jamaica_polygon_revised = jamaica_polygon_revised.to_crs(3448)\n",
    "jamaica_polygon_revised[\"geometry\"] = jamaica_polygon_revised.geometry.buffer(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_Layer_to_File(data, output_file, layer_name, driver):\n",
    "    data.to_file(output_file, layer=layer_name, driver=driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted and modified from https://github.com/thomas-fred/jam-coastal-protection\n",
    "\n",
    "def process_raster_to_clusters(threshold_m, eps, raster_file):\n",
    "    \"\"\"\n",
    "    Processes a raster file to identify clusters of depth values using DBSCAN, \n",
    "    converts them to polygons, and returns a GeoDataFrame for further processing.\n",
    "\n",
    "    Args:\n",
    "        threshold_m (float): Minimum depth value to include pixels in the analysis.\n",
    "        eps (float): DBSCAN epsilon parameter for cluster proximity.\n",
    "        raster_file (str): File path to the input raster file.\n",
    "\n",
    "    Returns:\n",
    "        gpd.GeoDataFrame: GeoDataFrame containing minimum enclosing polygons for clusters.\n",
    "    \"\"\"\n",
    "    # Open the raster file and read the first band (depth values)\n",
    "    raster = rasterio.open(raster_file)\n",
    "    depth_m = raster.read(1)\n",
    "    i, j = np.indices(depth_m.shape)  # Create row (i) and column (j) indices\n",
    "\n",
    "    # Extract the transformation and CRS from the raster\n",
    "    transform = from_origin(raster.bounds.left, raster.bounds.top, raster.res[0], raster.res[1])\n",
    "    original_crs = raster.crs\n",
    "\n",
    "    # Create a DataFrame and filter out pixels below the depth threshold\n",
    "    df = pd.DataFrame(data={\"i\": i.ravel(), \"j\": j.ravel(), \"depth_m\": depth_m.ravel()})\n",
    "    df = df[df[\"depth_m\"] > threshold_m]  # Keep only pixels above the threshold\n",
    "\n",
    "    if not df.empty:\n",
    "        # Initialize the DBSCAN clustering algorithm and fit the filtered data\n",
    "        db = sklearn.cluster.DBSCAN(eps=eps, min_samples=5)\n",
    "        X = df.loc[:, [\"i\", \"j\"]].to_numpy()  # Extract pixel coordinates\n",
    "        db.fit(X)\n",
    "\n",
    "        # Retrieve cluster labels and count the number of clusters\n",
    "        labels = db.labels_\n",
    "        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)  # Exclude noise points (-1)\n",
    "\n",
    "        if n_clusters >= 2:\n",
    "            # Create polygons from clusters and convert to a GeoDataFrame\n",
    "            gdf = create_polygons_from_clusters(labels, depth_m.shape, transform, original_crs, df)\n",
    "\n",
    "            # Reproject GeoDataFrame to EPSG:3448 coordinate reference system\n",
    "            gdf = gdf.to_crs(\"EPSG:3448\")\n",
    "\n",
    "            # Generate minimum enclosing polygons for each cluster\n",
    "            gdf = create_min_enclosing_polygons(gdf)\n",
    "            return gdf\n",
    "\n",
    "\n",
    "def create_polygons_from_clusters(labels, shape, transform, crs, df):\n",
    "    \"\"\"\n",
    "    Converts DBSCAN cluster labels into polygons and organizes them in a GeoDataFrame.\n",
    "\n",
    "    Args:\n",
    "        labels (np.ndarray): Cluster labels assigned by DBSCAN (-1 for noise).\n",
    "        shape (tuple): Shape of the raster (rows, cols).\n",
    "        transform (Affine): Transformation to convert pixel indices to coordinates.\n",
    "        crs (str): Coordinate reference system of the raster.\n",
    "        df (pd.DataFrame): DataFrame containing pixel indices and other relevant data.\n",
    "\n",
    "    Returns:\n",
    "        gpd.GeoDataFrame: GeoDataFrame containing polygons representing clusters.\n",
    "    \"\"\"\n",
    "    # Create an array to store cluster labels mapped to raster pixels\n",
    "    cluster_raster = np.full(shape, -1, dtype=np.int32)\n",
    "    cluster_raster[df[\"i\"], df[\"j\"]] = labels\n",
    "\n",
    "    polygons = []\n",
    "    for cluster_label in np.unique(labels):\n",
    "        if cluster_label == -1:\n",
    "            continue  # Skip noise points\n",
    "\n",
    "        # Identify the mask for the current cluster\n",
    "        cluster_mask = cluster_raster == cluster_label\n",
    "        contours = measure.find_contours(cluster_mask, level=0.5)\n",
    "\n",
    "        # Convert contours to polygons\n",
    "        for contour in contours:\n",
    "            polygon_coords = [\n",
    "                transform * (col, row) for row, col in contour\n",
    "            ]\n",
    "            polygon = Polygon(polygon_coords)\n",
    "            if polygon.is_valid:\n",
    "                polygons.append((cluster_label, polygon))\n",
    "\n",
    "    # Create a GeoDataFrame from the polygons\n",
    "    gdf = gpd.GeoDataFrame(polygons, columns=[\"cluster_label\", \"geometry\"], crs=crs)\n",
    "    return gdf\n",
    "\n",
    "\n",
    "def create_min_enclosing_polygons(gdf):\n",
    "    \"\"\"\n",
    "    Generates minimum enclosing polygons for each cluster in the GeoDataFrame.\n",
    "\n",
    "    Args:\n",
    "        gdf (gpd.GeoDataFrame): GeoDataFrame containing cluster polygons.\n",
    "\n",
    "    Returns:\n",
    "        gpd.GeoDataFrame: GeoDataFrame with updated minimum enclosing polygons.\n",
    "    \"\"\"\n",
    "    enclosing_polygons = []\n",
    "\n",
    "    for cluster_label in gdf[\"cluster_label\"].unique():\n",
    "        # Combine all polygons for the same cluster label into a single geometry\n",
    "        cluster_polygons = gdf[gdf[\"cluster_label\"] == cluster_label]\n",
    "        combined_geometry = unary_union(cluster_polygons.geometry)\n",
    "\n",
    "        # Create the minimum enclosing polygon (remove self-intersections with buffer)\n",
    "        enclosing_polygon = combined_geometry.buffer(0)\n",
    "        enclosing_polygons.append((cluster_label, enclosing_polygon))\n",
    "\n",
    "    # Create a new GeoDataFrame with the enclosing polygons\n",
    "    gdf_enclosing = gpd.GeoDataFrame(enclosing_polygons, columns=[\"cluster_label\", \"geometry\"], crs=gdf.crs)\n",
    "    return gdf_enclosing\n",
    "\n",
    "\n",
    "# Calling the function with appropriate parameters\n",
    "dbscan_flood_areas = process_raster_to_clusters(\n",
    "    threshold_m=depth_thresh,  # Minimum depth threshold\n",
    "    eps=eps_thresh,            # DBSCAN epsilon value\n",
    "    raster_file=raster_file    # Input raster file path\n",
    ")\n",
    "\n",
    "# Saving the output to a GeoPackage file\n",
    "layer_name = f\"dbscan_flood_areas_RP_{return_period}_eps_{eps_thresh}_thresh_{depth_thresh}\"\n",
    "add_Layer_to_File(dbscan_flood_areas, processing_file, layer_name, \"GPKG\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_polygons_along_edge(edge_layer, node_layer, buffer):\n",
    "    \"\"\"\n",
    "    Generate polygons (bounding boxes) aligned along edges in a GeoDataFrame.\n",
    "\n",
    "    For each edge, this function creates a rectangular bounding box centered on the edge's midpoint, \n",
    "    rotated to align with the edge's direction, and expanded vertically by the specified buffer.\n",
    "\n",
    "    Parameters:\n",
    "    - edge_layer (GeoDataFrame): GeoDataFrame containing edges (LineString) with 'from_id' and 'to_id' attributes.\n",
    "    - node_layer (GeoDataFrame): GeoDataFrame containing nodes (Point) with 'node_id' attributes.\n",
    "    - buffer (float): Additional height (vertical buffer) to be added to the bounding box, in the km.\n",
    "\n",
    "    Returns:\n",
    "    - GeoDataFrame: A GeoDataFrame containing rotated and translated bounding box polygons for each coastline segment.\n",
    "    \"\"\"\n",
    "    # List to store the resulting bounding boxes for each edge\n",
    "    bounding_boxes = []\n",
    "\n",
    "    # Lists to store additional attributes for the resulting GeoDataFrame\n",
    "    distances = []  # Store the length of each edge\n",
    "    midpoints = []  # Store the midpoint of each edge\n",
    "    angles = []     # Store the rotation angle for each edge\n",
    "\n",
    "    # Iterate through each edge in the edge layer\n",
    "    for _, edge in edge_layer.iterrows():\n",
    "        # Retrieve the 'from_id' and 'to_id' for the current edge\n",
    "        from_node_id = edge['from_id']\n",
    "        to_node_id = edge['to_id']\n",
    "        \n",
    "        # Get the coordinates of the start and end nodes using their IDs\n",
    "        from_node = node_layer[node_layer['node_id'] == from_node_id].geometry.iloc[0]\n",
    "        to_node = node_layer[node_layer['node_id'] == to_node_id].geometry.iloc[0]\n",
    "\n",
    "        # Skip edges where the start and end nodes are at the same location\n",
    "        if (round(from_node.x, 5) == round(to_node.x, 5)) and (round(from_node.y, 5) == round(to_node.y, 5)):\n",
    "            continue\n",
    "\n",
    "        # Calculate the straight-line distance (diagonal) between the two nodes\n",
    "        distance = from_node.distance(to_node)\n",
    "        distances.append(distance)\n",
    "\n",
    "        # Calculate the midpoint of the edge\n",
    "        midpoint = calculate_midpoint(from_node, to_node)\n",
    "        midpoints.append(midpoint)\n",
    "\n",
    "        # Calculate the angle of the edge relative to the horizontal axis\n",
    "        angle = calculate_angle(from_node, to_node)\n",
    "        angles.append(angle)\n",
    "\n",
    "        # Define the height of the bounding box (including buffer)\n",
    "        min_y = min(from_node.y, to_node.y)\n",
    "        max_y = max(from_node.y, to_node.y)\n",
    "        height = max_y - min_y + (buffer * 1000)  # Apply buffer to the height\n",
    "\n",
    "        # Define the width of the bounding box (distance between the two nodes)\n",
    "        min_x = min(from_node.x, to_node.x)\n",
    "        max_x = min_x + distance  # Adjust width to match edge length\n",
    "\n",
    "        # Create an initial rectangular bounding box\n",
    "        bounding_box = Polygon([\n",
    "            (min_x, min_y),\n",
    "            (min_x, min_y + height),  # Top-left corner\n",
    "            (max_x, min_y + height),  # Top-right corner\n",
    "            (max_x, min_y),           # Bottom-right corner\n",
    "            (min_x, min_y)            # Close back to bottom-left\n",
    "        ])\n",
    "\n",
    "        # Translate the bounding box to center it on the edge's midpoint\n",
    "        translated_bounding_box = translate_bounding_box(bounding_box, midpoint)\n",
    "\n",
    "        # Rotate the bounding box to align with the edge's orientation\n",
    "        rotated_bounding_box = rotate_bounding_box(translated_bounding_box, midpoint, angle)\n",
    "\n",
    "        # Append the final bounding box to the list\n",
    "        bounding_boxes.append(rotated_bounding_box)\n",
    "\n",
    "    # Convert the list of bounding boxes into a GeoDataFrame\n",
    "    bounding_boxes_gdf = gpd.GeoDataFrame(geometry=bounding_boxes, crs=edge_layer.crs)\n",
    "\n",
    "    # Add additional attributes (distance, angle) to the resulting GeoDataFrame\n",
    "    bounding_boxes_gdf['id'] = range(len(bounding_boxes_gdf))  # Unique ID for each polygon\n",
    "    bounding_boxes_gdf['distance'] = distances  # Distance between the nodes\n",
    "    bounding_boxes_gdf['angle'] = angles        # Angle of the edge in degrees\n",
    "\n",
    "    return bounding_boxes_gdf\n",
    "\n",
    "def calculate_midpoint(from_node, to_node):\n",
    "    \"\"\"\n",
    "    Calculate the midpoint between two nodes.\n",
    "\n",
    "    Parameters:\n",
    "    - from_node (Point): The starting node as a Shapely Point.\n",
    "    - to_node (Point): The ending node as a Shapely Point.\n",
    "\n",
    "    Returns:\n",
    "    - Point: The midpoint between the two nodes as a Shapely Point.\n",
    "    \"\"\"\n",
    "    midpoint_x = (from_node.x + to_node.x) / 2\n",
    "    midpoint_y = (from_node.y + to_node.y) / 2\n",
    "    return Point(midpoint_x, midpoint_y)\n",
    "\n",
    "def calculate_angle(from_node, to_node):\n",
    "    \"\"\"\n",
    "    Calculate the angle of the line connecting two nodes relative to the horizontal axis.\n",
    "\n",
    "    Parameters:\n",
    "    - from_node (Point): The starting node as a Shapely Point.\n",
    "    - to_node (Point): The ending node as a Shapely Point.\n",
    "\n",
    "    Returns:\n",
    "    - float: The angle in degrees, measured counterclockwise from the positive x-axis.\n",
    "    \"\"\"\n",
    "    delta_x = to_node.x - from_node.x\n",
    "    delta_y = to_node.y - from_node.y\n",
    "    angle_rad = math.atan2(delta_y, delta_x)\n",
    "    return math.degrees(angle_rad)\n",
    "\n",
    "def translate_bounding_box(bounding_box, midpoint):\n",
    "    \"\"\"\n",
    "    Translate a bounding box to center it on a specified midpoint.\n",
    "\n",
    "    Parameters:\n",
    "    - bounding_box (Polygon): The bounding box polygon to translate.\n",
    "    - midpoint (Point): The target center point for translation.\n",
    "\n",
    "    Returns:\n",
    "    - Polygon: The translated bounding box polygon.\n",
    "    \"\"\"\n",
    "    current_center_x = (bounding_box.bounds[0] + bounding_box.bounds[2]) / 2\n",
    "    current_center_y = (bounding_box.bounds[1] + bounding_box.bounds[3]) / 2\n",
    "    translate_x = midpoint.x - current_center_x\n",
    "    translate_y = midpoint.y - current_center_y\n",
    "    return affinity.translate(bounding_box, xoff=translate_x, yoff=translate_y)\n",
    "\n",
    "def rotate_bounding_box(bounding_box, midpoint, angle):\n",
    "    \"\"\"\n",
    "    Rotate a bounding box around a specified midpoint by a given angle.\n",
    "\n",
    "    Parameters:\n",
    "    - bounding_box (Polygon): The bounding box polygon to rotate.\n",
    "    - midpoint (Point): The center point for rotation.\n",
    "    - angle (float): The angle in degrees by which to rotate the bounding box.\n",
    "\n",
    "    Returns:\n",
    "    - Polygon: The rotated bounding box polygon.\n",
    "    \"\"\"\n",
    "    return affinity.rotate(bounding_box, angle, origin=(midpoint.x, midpoint.y))\n",
    "\n",
    "# Example usage\n",
    "edge_polygons = generate_polygons_along_edge(\n",
    "    edge_layer=coastline_edge_layer, \n",
    "    node_layer=coastline_node_layer,\n",
    "    buffer=inland_buffer_distance,\n",
    ")\n",
    "\n",
    "# Save the output to a file\n",
    "layer_name = f\"edge_polygons_depth_{inland_buffer_distance}\"\n",
    "add_Layer_to_File(edge_polygons, processing_file, layer_name, \"GPKG\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_max_zonal_stats(geopkg, raster_path, new_column_name):\n",
    "    \"\"\"\n",
    "    Add a new column to a GeoDataFrame with the maximum raster value for each polygon,\n",
    "    mimicking the functionality of the Zonal Statistics Tool in QGIS. \n",
    "    The GeoDataFrame is reprojected to match the raster CRS during the calculation \n",
    "    and reprojected back to its original CRS afterward.\n",
    "\n",
    "    Parameters:\n",
    "    - geopkg (GeoDataFrame): GeoDataFrame containing polygons for which zonal statistics will be calculated.\n",
    "    - raster_path (str): Path to the raster file used for calculating zonal statistics.\n",
    "    - new_column_name (str): Name of the new column to store the maximum raster value for each polygon.\n",
    "\n",
    "    Returns:\n",
    "    - GeoDataFrame: The updated GeoDataFrame with the new column containing maximum raster values.\n",
    "    \"\"\"\n",
    "    # Save the original CRS (Coordinate Reference System) of the GeoDataFrame for re-projection later\n",
    "    original_crs = geopkg.crs\n",
    "\n",
    "    # Open the raster file to retrieve its CRS\n",
    "    with rasterio.open(raster_path) as src:\n",
    "        raster_crs = src.crs\n",
    "\n",
    "    # Reproject the GeoDataFrame to match the raster CRS if their CRS do not align\n",
    "    if geopkg.crs != raster_crs:\n",
    "        geopkg = geopkg.to_crs(raster_crs)\n",
    "\n",
    "    # Ensure all geometries are valid (use a buffer of 0 as a fix for invalid geometries)\n",
    "    geopkg[\"geometry\"] = geopkg[\"geometry\"].buffer(0)\n",
    "\n",
    "    # Calculate zonal statistics to determine the maximum raster value within each polygon\n",
    "    stats = zonal_stats(\n",
    "        geopkg,          # GeoDataFrame containing polygons\n",
    "        raster_path,     # Path to the raster file\n",
    "        stats=\"max\",     # Calculate the maximum value for each zone\n",
    "        geojson_out=False,  # Do not output results in GeoJSON format\n",
    "        nodata=-9999     # Value to treat as NoData in the raster (adjust as necessary)\n",
    "    )\n",
    "\n",
    "    # Extract the maximum raster values from the zonal statistics results and add to a new column\n",
    "    geopkg[new_column_name] = [\n",
    "        stat[\"max\"] if stat[\"max\"] is not None else None  # Add max value or None if no data exists\n",
    "        for stat in stats\n",
    "    ]\n",
    "\n",
    "    # Reproject the GeoDataFrame back to its original CRS\n",
    "    if geopkg.crs != original_crs:\n",
    "        geopkg = geopkg.to_crs(original_crs)\n",
    "\n",
    "    return geopkg  # Return the updated GeoDataFrame with the new column\n",
    "\n",
    "\n",
    "# Call the function to calculate and add the maximum flood height from the raster\n",
    "edge_polygons = add_max_zonal_stats(\n",
    "    geopkg=edge_polygons,\n",
    "    raster_path=raster_file,\n",
    "    new_column_name=\"max_flood_height\"\n",
    ")\n",
    "\n",
    "# Save the updated GeoDataFrame back to the file, ensuring CRS consistency\n",
    "layer_name = f\"edge_polygons_depth_{inland_buffer_distance}\"\n",
    "add_Layer_to_File(edge_polygons, processing_file, layer_name, \"GPKG\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296\n"
     ]
    }
   ],
   "source": [
    "def join_polygons_by_flood_height(gdf, flood_height_threshold, area_limit, group_size):\n",
    "    \"\"\"\n",
    "    Groups polygons in a GeoDataFrame based on their maximum flood height, with additional grouping \n",
    "    conditions based on area size and maximum group size. \n",
    "    \n",
    "    Polygons are joined together if their flood height exceeds a specified threshold or if their combined area\n",
    "    exceeds a specified limit.\n",
    "\n",
    "    Parameters:\n",
    "    - gdf (GeoDataFrame): Input GeoDataFrame containing polygons and their associated maximum flood heights\n",
    "    - flood_height_threshold (float): The flood height threshold above which polygons are grouped together\n",
    "    - area_limit (float): The area limit for groups; groups with areas exceeding this limit are finalized\n",
    "    - group_size (int): The  minimum number of polygons that can make up a group\n",
    "\n",
    "    Returns:\n",
    "    - GeoDataFrame: A new GeoDataFrame containing the grouped polygons, with a new 'max_flood_height' column\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize lists to store results for each group\n",
    "    groups = []  # List of combined polygons (grouped geometries)\n",
    "    heights = []  # List of maximum flood heights for each group\n",
    "    ids = []  # List of original polygon IDs for each group\n",
    "    \n",
    "    # Initialize variables to track the current group being processed\n",
    "    current_group = []  # List of polygons in the current group\n",
    "    current_heights = []  # List of max flood heights for the current group\n",
    "    current_ids = []  # List of original IDs for the current group\n",
    "    is_below_threshold = None  # Flag indicating whether the current polygon is below the threshold\n",
    "\n",
    "    # Iterate over each polygon in the GeoDataFrame\n",
    "    for index, row in gdf.iterrows():\n",
    "        if row[\"max_flood_height\"] > flood_height_threshold:\n",
    "            # If the polygon's flood height exceeds the threshold, handle grouping\n",
    "            if is_below_threshold is True or (current_group and unary_union(current_group).area > area_limit):\n",
    "                # Finalize the current group if necessary (area limit exceeded or threshold state changed)\n",
    "                if len(current_group) < group_size and groups:\n",
    "                    # Add the current polygon to the previous group (if the group reached max size)\n",
    "                    groups[-1] = unary_union([groups[-1], current_group[0]]).convex_hull\n",
    "                    heights[-1] = max(heights[-1], current_heights[0])  # Update max height\n",
    "                    ids[-1].extend(current_ids)  # Add original IDs to the group\n",
    "                else:\n",
    "                    # Finalize the group normally by combining geometries\n",
    "                    combined_geometry = unary_union(current_group)\n",
    "                    groups.append(combined_geometry.convex_hull)\n",
    "                    heights.append(max(current_heights))  # Store the max flood height for the group\n",
    "                    ids.append(current_ids)  # Store the original IDs for the group\n",
    "\n",
    "                # Reset the current group and associated data for the next set of polygons\n",
    "                current_group = []\n",
    "                current_heights = []\n",
    "                current_ids = []\n",
    "\n",
    "            # Switch to above-threshold mode\n",
    "            is_below_threshold = False\n",
    "            current_group.append(row[\"geometry\"])  # Add the polygon's geometry to the current group\n",
    "            current_heights.append(row[\"max_flood_height\"])  # Add max flood height to the current group\n",
    "            current_ids.append(row[\"id\"])  # Add the polygon's ID to the current group\n",
    "\n",
    "        else:  # Polygons below or equal to the flood height threshold\n",
    "            # If the polygon is below the threshold, handle the grouping similarly\n",
    "            if is_below_threshold is False or (current_group and unary_union(current_group).area > area_limit):\n",
    "                # Finalize the current group if necessary (area limit exceeded or threshold state changed)\n",
    "                if len(current_group) < group_size and groups:\n",
    "                    # Add the current polygon to the previous group\n",
    "                    groups[-1] = unary_union([groups[-1], current_group[0]]).convex_hull\n",
    "                    heights[-1] = max(heights[-1], current_heights[0])  # Update max height\n",
    "                    ids[-1].extend(current_ids)  # Add original IDs to the group\n",
    "                else:\n",
    "                    # Finalize the group normally by combining geometries\n",
    "                    combined_geometry = unary_union(current_group)\n",
    "                    groups.append(combined_geometry.convex_hull)\n",
    "                    heights.append(max(current_heights))  # Store the max flood height for the group\n",
    "                    ids.append(current_ids)  # Store the original IDs for the group\n",
    "\n",
    "                # Reset the current group and associated data for the next set of polygons\n",
    "                current_group = []\n",
    "                current_heights = []\n",
    "                current_ids = []\n",
    "\n",
    "            # Switch to below-threshold mode\n",
    "            is_below_threshold = True\n",
    "            current_group.append(row[\"geometry\"])  # Add the polygon's geometry to the current group\n",
    "            current_heights.append(row[\"max_flood_height\"])  # Add max flood height to the current group\n",
    "            current_ids.append(row[\"id\"])  # Add the polygon's ID to the current group\n",
    "\n",
    "    # Finalize the last group if any polygons remain\n",
    "    if current_group:\n",
    "        if len(current_group) < group_size and groups:\n",
    "            # Add the current polygon to the previous group (if group size exceeded)\n",
    "            groups[-1] = unary_union([groups[-1], current_group[0]]).convex_hull\n",
    "            heights[-1] = max(heights[-1], current_heights[0])  # Update max height\n",
    "            ids[-1].extend(current_ids)  # Add original IDs to the group\n",
    "        else:\n",
    "            # Finalize the group normally by combining geometries\n",
    "            combined_geometry = unary_union(current_group)\n",
    "            groups.append(combined_geometry.convex_hull)\n",
    "            heights.append(max(current_heights))  # Store max flood height\n",
    "            ids.append(current_ids)  # Store original IDs\n",
    "\n",
    "    # Convert the grouped data back into a GeoDataFrame\n",
    "    result_gdf = gpd.GeoDataFrame(\n",
    "        {\n",
    "            \"geometry\": gpd.GeoSeries(groups),  # Combined geometries for each group\n",
    "            \"max_flood_height\": heights,  # Max flood heights for each group\n",
    "            \"original_ids\": ids,  # Original IDs for each group\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Add a unique ID field to the result GeoDataFrame\n",
    "    result_gdf[\"id\"] = range(len(result_gdf))\n",
    "\n",
    "    # Set the CRS (Coordinate Reference System) to match the original GeoDataFrame\n",
    "    result_gdf.set_crs(gdf.crs, inplace=True)\n",
    "\n",
    "    return result_gdf  # Return the GeoDataFrame with grouped polygons\n",
    "\n",
    "\n",
    "def order_edges_around_island(edge_layer):\n",
    "    \"\"\"\n",
    "    Order edges around an island, starting with edge_0 and following the\n",
    "    connections based on from_id and to_id.\n",
    "\n",
    "    Parameters:\n",
    "    - edge_layer (GeoDataFrame): GeoDataFrame containing edges with 'id', 'from_id', and 'to_id' columns\n",
    "\n",
    "    Returns:\n",
    "    - List[str]: List of associated bounding box IDs in the order the edges are traced around the island\n",
    "    \"\"\"\n",
    "    # Initialize the ordered list with the first edge (edge_0)\n",
    "    ordered_edges = []\n",
    "\n",
    "    # Create a dictionary to lookup edges by their 'from_id' for efficient access\n",
    "    edges_by_from_id = {\n",
    "        edge['from_id']: edge\n",
    "        for _, edge in edge_layer.iterrows()\n",
    "    }\n",
    "\n",
    "    # Start with the edge having 'id' equal to 'edge_0'\n",
    "    current_edge = edge_layer[edge_layer['id'] == 'edge_0'].iloc[0]\n",
    "    ordered_edges.append(current_edge['rectangle_id'])  # Append the first edge's ID\n",
    "\n",
    "    # Track the starting node to detect when we've completed a loop\n",
    "    start_node = current_edge['from_id']\n",
    "\n",
    "    # Continue looping through the edges, following the 'to_id' until we return to the starting node\n",
    "    while True:\n",
    "        # Find the next edge based on the current edge's 'to_id'\n",
    "        next_from_id = current_edge['to_id']\n",
    "\n",
    "        # Break the loop if we return to the starting node (complete the loop)\n",
    "        if next_from_id == start_node:\n",
    "            break\n",
    "\n",
    "        # Look up the next edge using the 'from_id' as the key in the dictionary\n",
    "        current_edge = edges_by_from_id[next_from_id]\n",
    "        ordered_edges.append(current_edge['rectangle_id'])  # Add the next edge's ID to the ordered list\n",
    "\n",
    "    return ordered_edges  # Return the ordered list of edge IDs\n",
    "\n",
    "\n",
    "# Call the function to get the ordered list of edge IDs\n",
    "ordered_edge_ids = order_edges_around_island(\n",
    "    edge_layer = coastline_edge_layer\n",
    ")\n",
    "\n",
    "# Filter the original polygon layer to include only the ordered edge IDs\n",
    "ordered_edge_ids = [eid for eid in ordered_edge_ids if eid in edge_polygons[\"id\"].values]\n",
    "input_polygons = edge_polygons.set_index(\"id\").loc[ordered_edge_ids].reset_index()\n",
    "\n",
    "# Parameters for grouping polygons\n",
    "flood_height_threshold = 0  # Flood height threshold above which polygons will be grouped\n",
    "area_limit = 8_000_000  # Area limit for group finalization\n",
    "group_size = 2  # Minimum number of polygonsneeded to make up a group\n",
    "\n",
    "# Call the function to join polygons based on flood height and area conditions\n",
    "joined_polygons = join_polygons_by_flood_height(\n",
    "    gdf = input_polygons, \n",
    "    flood_height_threshold = flood_height_threshold, \n",
    "    area_limit = area_limit, \n",
    "    group_size = group_size\n",
    ")\n",
    "\n",
    "print (len(joined_polygons))\n",
    "# Define layer name for the final output\n",
    "layer_name = f\"joined_edge_polygons_flood_{flood_height_threshold}_area_{area_limit}_group_{group_size}\"\n",
    "\n",
    "# Save the result back to a file, ensuring CRS consistency\n",
    "add_Layer_to_File(joined_polygons, processing_file, layer_name, \"GPKG\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_overlapping_polygons(gdf, overlap_threshold):\n",
    "    \"\"\"\n",
    "    Iteratively merges overlapping polygons that overlap by more than the specified threshold.\n",
    "    \n",
    "    Args:\n",
    "        gdf (GeoDataFrame): GeoDataFrame containing polygons with 'geometry' and 'max_flood_height' fields.\n",
    "        overlap_threshold (float): The minimum percentage of overlap required to trigger a merge (e.g., 0.35 for 35% overlap).\n",
    "    \n",
    "    Returns:\n",
    "        GeoDataFrame: A new GeoDataFrame with merged polygons until no pair overlaps by more than the threshold.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure that the CRS (Coordinate Reference System) is defined for the input data\n",
    "    if gdf.crs is None:\n",
    "        gdf.set_crs(\"EPSG:3097\", inplace=True)  # Set CRS to Jamaica Metric Grid, adjust if necessary\n",
    "\n",
    "    # Helper function to perform one pass of merging overlapping polygons\n",
    "    def merge_once(gdf, overlap_threshold):\n",
    "        \"\"\"\n",
    "        Performs one pass over the GeoDataFrame to merge overlapping polygons.\n",
    "        Merges polygons whose intersection area exceeds the given overlap threshold.\n",
    "        \n",
    "        Args:\n",
    "            gdf (GeoDataFrame): GeoDataFrame to process.\n",
    "            overlap_threshold (float): The overlap threshold to trigger a merge.\n",
    "        \n",
    "        Returns:\n",
    "            GeoDataFrame: A new GeoDataFrame with merged polygons from this pass.\n",
    "        \"\"\"\n",
    "        merged = []  # List to hold merged geometries\n",
    "        indices_to_merge = set()  # Set to keep track of merged polygons\n",
    "\n",
    "        # Loop through all polygons in the GeoDataFrame to check for overlaps\n",
    "        for i, geom1 in enumerate(gdf.geometry):\n",
    "            if i in indices_to_merge:\n",
    "                continue  # Skip geometries that are already merged\n",
    "\n",
    "            for j, geom2 in enumerate(gdf.geometry):\n",
    "                if i >= j or j in indices_to_merge:\n",
    "                    continue  # Avoid redundant checks and already-merged geometries\n",
    "\n",
    "                # Check if the two polygons intersect\n",
    "                if geom1.intersects(geom2):\n",
    "                    intersection = geom1.intersection(geom2)\n",
    "                    if intersection.is_empty:\n",
    "                        continue  # Skip if the intersection is empty\n",
    "\n",
    "                    # Calculate the area of the intersection and compare with the polygons' areas\n",
    "                    area1 = geom1.area\n",
    "                    area2 = geom2.area\n",
    "                    intersection_area = intersection.area\n",
    "\n",
    "                    # Check if the overlap exceeds the threshold\n",
    "                    if (intersection_area / min(area1, area2)) > overlap_threshold:\n",
    "                        # Merge the two polygons by creating a convex hull around the union\n",
    "                        new_geom = unary_union([geom1, geom2]).convex_hull\n",
    "                        new_height = max(gdf.loc[i, \"max_flood_height\"], gdf.loc[j, \"max_flood_height\"])\n",
    "\n",
    "                        # Store the merged geometry and its max flood height\n",
    "                        merged.append({\n",
    "                            \"geometry\": new_geom,\n",
    "                            \"max_flood_height\": new_height\n",
    "                        })\n",
    "                        indices_to_merge.update([i, j])  # Mark the merged polygons\n",
    "\n",
    "                        break  # Stop checking other polygons for the current geometry once merged\n",
    "\n",
    "        # Retain the geometries that were not merged\n",
    "        non_merged = gdf.loc[~gdf.index.isin(indices_to_merge)]\n",
    "\n",
    "        # Combine the merged geometries with the non-overlapping ones\n",
    "        merged_gdf = gpd.GeoDataFrame(\n",
    "            {\n",
    "                \"geometry\": [m[\"geometry\"] for m in merged],\n",
    "                \"max_flood_height\": [m[\"max_flood_height\"] for m in merged],\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Ensure CRS is explicitly set for the merged geometries\n",
    "        merged_gdf.set_crs(gdf.crs, inplace=True)\n",
    "\n",
    "        # Concatenate the non-overlapping geometries with the merged ones\n",
    "        final_gdf = gpd.GeoDataFrame(pd.concat([non_merged, merged_gdf], ignore_index=True))\n",
    "\n",
    "        # Set CRS again after concatenation to ensure consistency\n",
    "        final_gdf.set_crs(gdf.crs, inplace=True)\n",
    "\n",
    "        return final_gdf\n",
    "\n",
    "    # Iteratively merge polygons until no more overlaps are found\n",
    "    prev_gdf = gdf\n",
    "    while True:\n",
    "        new_gdf = merge_once(prev_gdf, overlap_threshold)\n",
    "        if len(new_gdf) == len(prev_gdf):  # No changes (no more overlaps)\n",
    "            break\n",
    "        prev_gdf = new_gdf  # Update the GeoDataFrame for the next iteration\n",
    "\n",
    "    # Ensure final CRS consistency\n",
    "    prev_gdf.set_crs(gdf.crs, inplace=True)\n",
    "\n",
    "    # Reassign unique IDs starting from 1 for the merged polygons\n",
    "    prev_gdf = prev_gdf.reset_index(drop=True)  # Reset the index\n",
    "    prev_gdf['id'] = prev_gdf.index + 1  # Assign new unique IDs based on the index\n",
    "\n",
    "    return prev_gdf\n",
    "\n",
    "overlap = 0.39\n",
    "\n",
    "# Apply the merging function to resolve overlaps\n",
    "s_cape_flood_areas = merge_overlapping_polygons(\n",
    "    gdf = joined_polygons, \n",
    "    overlap_threshold = overlap\n",
    ")\n",
    "\n",
    "# Create a layer name dynamically for the merged flood areas\n",
    "layer_name = f\"scape_flood_areas_flood_{flood_height_threshold}_area_{area_limit}_group_{group_size}_overlap_{overlap}\"\n",
    "\n",
    "# Add the merged polygons layer to the file\n",
    "add_Layer_to_File(s_cape_flood_areas, processing_file, layer_name, \"GPKG\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trimmed_convex_hull(polygons):\n",
    "    \"\"\"\n",
    "    Create a convex hull encompassing the given polygons by computing the convex hull\n",
    "    from their union.\n",
    "    \n",
    "    Args:\n",
    "        polygons (list): A list of geometries (polygons) to create the convex hull around.\n",
    "    \n",
    "    Returns:\n",
    "        geometry: The convex hull geometry encompassing all the input polygons.\n",
    "    \"\"\"\n",
    "    # Combine the polygons and compute the convex hull of their union\n",
    "    combined = unary_union(polygons)  # Union of all polygons\n",
    "    convex_hull = combined.convex_hull  # Convex hull of the union\n",
    "\n",
    "    return convex_hull\n",
    "\n",
    "def connect_polygons_with_convex_hull(flood_polygons):\n",
    "    \"\"\"\n",
    "    Generate a trimmed convex hull connecting the given polygons and save the result to a new GeoPackage.\n",
    "    \n",
    "    Args:\n",
    "        flood_polygons (GeoDataFrame): GeoDataFrame containing the polygons to connect with a convex hull.\n",
    "    \n",
    "    Returns:\n",
    "        GeoDataFrame: A GeoDataFrame containing the trimmed convex hull as a new polygon.\n",
    "    \"\"\"\n",
    "    # Use the provided GeoDataFrame of flood polygons\n",
    "    gdf = flood_polygons\n",
    "\n",
    "    # In this case, all polygons are selected, but this can be modified for specific selections\n",
    "    selected_gdf = gdf\n",
    "\n",
    "    # Ensure the geometry column exists and convert to a list of geometries\n",
    "    polygons = selected_gdf.geometry.tolist()  # List of geometries for hull creation\n",
    "\n",
    "    # Create the trimmed convex hull using the helper function\n",
    "    trimmed_hull = create_trimmed_convex_hull(polygons)\n",
    "\n",
    "    # Prepare a new GeoDataFrame for the trimmed convex hull\n",
    "    new_row = gpd.GeoDataFrame({\n",
    "        'id': ['trimmed_hull'],  # Assign an ID to the new hull\n",
    "        'geometry': [trimmed_hull]  # The geometry of the convex hull\n",
    "    }, crs=gdf.crs)  # Ensure the new row uses the same CRS as the input GeoDataFrame\n",
    "\n",
    "    return new_row\n",
    "\n",
    "# Generate the convex hull for the flood polygons\n",
    "jamaica_convex_hull = connect_polygons_with_convex_hull(\n",
    "    flood_polygons = s_cape_flood_areas\n",
    ")\n",
    "\n",
    "# Define the layer name for the convex hull and save it to the GeoPackage\n",
    "layer_name = \"jamaica_convex\"\n",
    "add_Layer_to_File(jamaica_convex_hull, processing_file, layer_name, \"GPKG\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_voronoi(jamaica_polygon, jam_contour, coastline_polygons):\n",
    "    \"\"\"\n",
    "    Create Voronoi tessellation based on the centroids of smaller polygons (coastline),\n",
    "    clipped to the boundary of a larger polygon (Jamaica's convex hull).\n",
    "    \n",
    "    Args:\n",
    "        jamaica_polygon (GeoDataFrame): The larger polygon (e.g., Jamaica's convex hull).\n",
    "        jam_contour (GeoDataFrame): A GeoDataFrame containing the exact contour of the Jamaica.\n",
    "        coastline_polygons (GeoDataFrame): The smaller polygons (e.g., coastline polygons) for which Voronoi regions will be calculated.\n",
    "\n",
    "    Returns:\n",
    "        GeoDataFrame: Clipped Voronoi polygons inside the larger polygon with associated centroid IDs.\n",
    "        GeoDataFrame: Centroids of the smaller polygons used for Voronoi tessellation with a 'c_id' column.\n",
    "    \"\"\"\n",
    "    # Load the larger polygon (assumed to be Jamaica's convex hull)\n",
    "    convex_gdf = jamaica_polygon\n",
    "    larger_polygon = convex_gdf.iloc[0].geometry  # Assuming the first row contains the larger polygon\n",
    "\n",
    "    # Load the coastline polygons (smaller polygons)\n",
    "    gdf = coastline_polygons\n",
    "    smaller_polygons = gdf\n",
    "\n",
    "    # Ensure the CRS is consistent for all GeoDataFrames\n",
    "    smaller_polygons.crs = gdf.crs  # Ensure the coordinate reference system is consistent\n",
    "\n",
    "    #-------------------------------------------------\n",
    "    # Extract centroids of the smaller polygons to serve as Voronoi seed points\n",
    "    centroids = smaller_polygons.geometry.centroid\n",
    "\n",
    "    # Add a 'c_id' column to the centroids, which identifies the original polygon\n",
    "    centroid_ids = smaller_polygons['id'] if 'id' in smaller_polygons.columns else range(len(centroids))\n",
    "    centroid_gdf = gpd.GeoDataFrame({'c_id': centroid_ids}, geometry=centroids, crs=smaller_polygons.crs)\n",
    "\n",
    "    # Function to adjust points to be within or touching a larger polygon\n",
    "    def adjust_point_to_polygon(point, polygon):\n",
    "        \"\"\"\n",
    "        Adjust the point to be within or touching the boundary of the larger polygon.\n",
    "        If the point is outside, it will be moved to the nearest boundary point.\n",
    "        \n",
    "        Args:\n",
    "            point (Point): The point (centroid) to adjust.\n",
    "            polygon (Polygon): The larger polygon representing Jamaica's exact shape.\n",
    "        \n",
    "        Returns:\n",
    "            Point: The adjusted point that is within or on the boundary of the polygon.\n",
    "        \"\"\"\n",
    "        if polygon.contains(point) or polygon.touches(point):\n",
    "            return point\n",
    "        else:\n",
    "            # Find the nearest point on the boundary\n",
    "            nearest_boundary_point = nearest_points(point, polygon.boundary)[1]\n",
    "            # Move the point slightly closer to the boundary\n",
    "            adjusted_point = Point(\n",
    "                point.x + (nearest_boundary_point.x - point.x),\n",
    "                point.y + (nearest_boundary_point.y - point.y)\n",
    "            )\n",
    "            # Ensure the adjusted point is within or touching the polygon\n",
    "            if polygon.contains(adjusted_point) or polygon.touches(adjusted_point):\n",
    "                return adjusted_point\n",
    "            # As a fallback, snap directly to the nearest boundary point\n",
    "            return nearest_boundary_point\n",
    "\n",
    "    # Adjust centroids to be within or touching the larger polygon (Jamaica's boundary)\n",
    "    adjusted_centroids = [adjust_point_to_polygon(pt, jam_contour.iloc[0].geometry) for pt in centroids]\n",
    "\n",
    "    # Convert the adjusted centroids back to a GeoSeries for consistent geometry\n",
    "    adjusted_centroids_gs = gpd.GeoSeries(adjusted_centroids, crs=centroid_gdf.crs)\n",
    "\n",
    "    # Update the GeoDataFrame with adjusted centroids\n",
    "    centroid_gdf.geometry = adjusted_centroids_gs\n",
    "\n",
    "    # Create a MultiPoint object from the adjusted centroids for Voronoi tessellation\n",
    "    seed_points = MultiPoint(adjusted_centroids)\n",
    "\n",
    "    # Perform Voronoi tessellation using the adjusted centroids as seed points\n",
    "    voronoi = voronoi_diagram(seed_points)\n",
    "\n",
    "    #-------------------------------------------------\n",
    "    # Clip the Voronoi regions to the boundary of the larger polygon (Jamaica's convex hull)\n",
    "    clipped_regions = [region.intersection(larger_polygon) for region in voronoi.geoms]\n",
    "\n",
    "    # Filter out invalid or empty regions, keeping only valid polygons\n",
    "    valid_regions = [region for region in clipped_regions if not region.is_empty and isinstance(region, Polygon)]\n",
    "\n",
    "    # Match centroids to Voronoi regions based on proximity (the closest centroid to each Voronoi region)\n",
    "    associated_centroid_ids = []\n",
    "    for region in valid_regions:\n",
    "        # Find the centroid nearest to the region's centroid\n",
    "        region_centroid = region.centroid\n",
    "        nearest_centroid = centroids.distance(region_centroid).idxmin()\n",
    "        associated_centroid_ids.append(centroid_gdf.loc[nearest_centroid, 'c_id'])\n",
    "\n",
    "    # Create a GeoDataFrame for the clipped Voronoi regions with associated centroid IDs\n",
    "    clipped_gdf = gpd.GeoDataFrame(\n",
    "        {'id': range(len(valid_regions)),  # Assign a unique ID to each Voronoi region\n",
    "         'centroid_id': associated_centroid_ids},  # Store the associated centroid ID for each region\n",
    "        geometry=valid_regions,\n",
    "        crs=smaller_polygons.crs  # Use the same CRS as the smaller polygons\n",
    "    )\n",
    "\n",
    "    # Return both the clipped Voronoi regions and the centroids GeoDataFrame\n",
    "    return clipped_gdf, centroid_gdf\n",
    "\n",
    "# Load contour and coastline polygons from the input files\n",
    "jam_contour = gpd.read_file(input_file, layer=\"jam\") \n",
    "\n",
    "# Create the Voronoi polygons clipped to Jamaica's convex hull\n",
    "voronoi_polygons, voronoi_centroid_points = create_voronoi(\n",
    "    jamaica_polygon = jamaica_convex_hull, \n",
    "    jam_contour = jam_contour,\n",
    "    coastline_polygons = s_cape_flood_areas,\n",
    ")\n",
    "\n",
    "# Save the Voronoi polygons and centroid points to a GeoPackage\n",
    "layer_name = \"voronoi_polygons\"\n",
    "add_Layer_to_File(voronoi_polygons, processing_file, layer_name, \"GPKG\")\n",
    "\n",
    "layer_name = \"voronoi_centroid_points\"\n",
    "add_Layer_to_File(voronoi_centroid_points, processing_file, layer_name, \"GPKG\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted and modified from https://github.com/thomas-fred/jam-coastal-protection\n",
    "\n",
    "def combine_floods_within_voronoi(voronoi_polygons: gpd.GeoDataFrame,flood_polygons: gpd.GeoDataFrame,voronoi_points: gpd.GeoDataFrame,max_distance: float) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Combines all parts of flood polygons that intersect with each Voronoi polygon,\n",
    "    but remove any parts whose centroids are farther than a given distance from\n",
    "    the corresponding Voronoi point.\n",
    "\n",
    "    Args:\n",
    "        voronoi_polygons (gpd.GeoDataFrame): GeoDataFrame containing the Voronoi polygons.\n",
    "        flood_polygons (gpd.GeoDataFrame): GeoDataFrame containing the flood polygons to combine.\n",
    "        voronoi_points (gpd.GeoDataFrame): GeoDataFrame containing the Voronoi points with a 'fid' column that matches the 'centroid_id' column.\n",
    "        max_distance (float): Maximum allowed distance (in the same CRS units) between a polygon's centroid and the corresponding Voronoi point.\n",
    "\n",
    "    Returns:\n",
    "        gpd.GeoDataFrame: A GeoDataFrame containing the combined flood polygons for each Voronoi polygon,\n",
    "                           with the flood areas merged based on their proximity to the Voronoi points.\n",
    "    \"\"\"\n",
    "    combined_polygons = []  # List to store resulting geometries\n",
    "    voronoi_ids = []  # List to store Voronoi IDs (optional, for tracking)\n",
    "\n",
    "    # Loop through each Voronoi polygon\n",
    "    for voronoi_index, voronoi in voronoi_polygons.iterrows():\n",
    "        voronoi_geom = voronoi.geometry\n",
    "        centroid_id = voronoi[\"centroid_id\"]  # Get the Voronoi polygon's centroid ID\n",
    "\n",
    "        # Get the corresponding Voronoi point based on the centroid ID\n",
    "        voronoi_point = voronoi_points[voronoi_points[\"c_id\"] == centroid_id]\n",
    "        if voronoi_point.empty:\n",
    "            continue  # Skip if no matching Voronoi point is found\n",
    "\n",
    "        voronoi_point_geom = voronoi_point.geometry.iloc[0]\n",
    "\n",
    "        # Find all flood polygons that intersect with the current Voronoi polygon\n",
    "        intersecting_floods = flood_polygons[flood_polygons.intersects(voronoi_geom)]\n",
    "\n",
    "        if not intersecting_floods.empty:\n",
    "            # Combine the intersecting flood polygons within the Voronoi polygon\n",
    "            combined_geom = intersecting_floods.intersection(voronoi_geom).union_all()\n",
    "\n",
    "            # Filter out parts of the combined geometry that are too far from the Voronoi point\n",
    "            if isinstance(combined_geom, MultiPolygon):\n",
    "                # If the combined geometry is a MultiPolygon, check each part\n",
    "                filtered_parts = [\n",
    "                    part for part in combined_geom.geoms\n",
    "                    if part.centroid.distance(voronoi_point_geom) <= max_distance\n",
    "                ]\n",
    "                combined_geom = MultiPolygon(filtered_parts) if filtered_parts else None\n",
    "            elif combined_geom.centroid.distance(voronoi_point_geom) > max_distance:\n",
    "                # If the combined geometry is a single polygon, check its centroid distance\n",
    "                combined_geom = None\n",
    "\n",
    "            if combined_geom:\n",
    "                # Append the filtered geometry and its Voronoi index\n",
    "                combined_polygons.append(combined_geom)\n",
    "                voronoi_ids.append(voronoi_index)\n",
    "\n",
    "    # Create a GeoDataFrame for the combined polygons\n",
    "    result_gdf = gpd.GeoDataFrame({\n",
    "        \"voronoi_id\": voronoi_ids,  # Include the Voronoi IDs\n",
    "        \"geometry\": combined_polygons  # Include the geometries\n",
    "    }, crs=voronoi_polygons.crs)  # Set CRS to match Voronoi polygons\n",
    "\n",
    "    return result_gdf  # Return the combined flood polygons GeoDataFrame\n",
    "\n",
    "\n",
    "def linestring_intersect_polygons(default_inland_distance: float, coast: gpd.GeoDataFrame, polygons: gpd.GeoDataFrame, flood_polygons: gpd.GeoDataFrame) -> (gpd.GeoDataFrame, gpd.GeoDataFrame):\n",
    "    \"\"\"\n",
    "    Sections the coastline based on intersections with each Voronoi polygon and individually \n",
    "    buffers the new linestring segments until they completely enclose the flood polygon \n",
    "    within the corresponding Voronoi section.\n",
    "\n",
    "    Also returns a GeoDataFrame of intersections between the linestring and Voronoi polygons before any buffering.\n",
    "\n",
    "    Args:\n",
    "        default_inland_distance (float): Buffer radius in kilometers.\n",
    "        coast (gpd.GeoDataFrame): GeoDataFrame containing the coastline geometry to be buffered.\n",
    "        polygons (gpd.GeoDataFrame): GeoDataFrame of Voronoi polygons to check intersections.\n",
    "        flood_polygons (gpd.GeoDataFrame): GeoDataFrame of flood polygons with a 'voronoi_id' column to associate with Voronoi polygons.\n",
    "\n",
    "    Returns:\n",
    "        tuple: \n",
    "            - GeoDataFrame containing the intersections of the linestring and Voronoi polygons before buffering.\n",
    "            - GeoDataFrame containing the final intersected and buffered geometries, with Voronoi IDs.\n",
    "    \"\"\"\n",
    "    # Prepare lists to store intersections and final buffered geometries\n",
    "    initial_intersections = []\n",
    "    final_intersections = []\n",
    "\n",
    "    # Create a GeoDataFrame containing the union of all coastline geometries\n",
    "    linestring = gpd.GeoDataFrame({\"id\": [0], \"geometry\": [coast.geometry.union_all()]})\n",
    "    linestring.crs = coast.crs  # Set CRS to match the coast's CRS\n",
    "\n",
    "    # Iterate over each Voronoi polygon\n",
    "    for idx, poly in polygons.iterrows():\n",
    "        # Check if the linestring intersects with the Voronoi polygon\n",
    "        if linestring.geometry.iloc[0].intersects(poly.geometry):\n",
    "            # Perform the intersection between the linestring and the Voronoi polygon\n",
    "            intersection = linestring.geometry.iloc[0].intersection(poly.geometry)\n",
    "            \n",
    "            # Append the raw intersection with the associated Voronoi ID\n",
    "            initial_intersections.append({\n",
    "                \"geometry\": intersection,\n",
    "                \"voronoi_id\": idx\n",
    "            })\n",
    "\n",
    "            # Buffer the intersection piece separately based on the provided inland distance\n",
    "            buffered_intersection = intersection.buffer(default_inland_distance * 1_000)\n",
    "            \n",
    "            # Find the corresponding flood polygon that has the same 'voronoi_id'\n",
    "            flood_polygon = flood_polygons[flood_polygons['voronoi_id'] == idx]\n",
    "            \n",
    "            if not flood_polygon.empty:\n",
    "                flood_geometry = flood_polygon.geometry.iloc[0]\n",
    "                \n",
    "                # Start iteratively buffering the intersection from 0.5 km and increase by 0.1 km\n",
    "                current_buffer_radius = 0.5 * 1_000  # Start with a buffer radius of 0.5 km\n",
    "                while not buffered_intersection.contains(flood_geometry):\n",
    "                    # Increase buffer by 0.1 km at each iteration until it contains the flood geometry\n",
    "                    current_buffer_radius += 0.1 * 1_000\n",
    "                    buffered_intersection = intersection.buffer(current_buffer_radius)\n",
    "            \n",
    "            # Perform the final intersection with the flood polygon\n",
    "            final_intersection = buffered_intersection.intersection(poly.geometry)\n",
    "            \n",
    "            # Append the final intersection with its associated Voronoi ID to the list\n",
    "            final_intersections.append({\n",
    "                \"geometry\": final_intersection,\n",
    "                \"voronoi_id\": idx\n",
    "            })\n",
    "    \n",
    "    # Create GeoDataFrames for initial and final intersections\n",
    "    initial_intersections_gdf = gpd.GeoDataFrame(initial_intersections, crs=polygons.crs)\n",
    "    final_intersections_gdf = gpd.GeoDataFrame(final_intersections, crs=polygons.crs)\n",
    "\n",
    "    return initial_intersections_gdf, final_intersections_gdf\n",
    "\n",
    "\n",
    "\n",
    "# Combine flood polygons for each Voronoi polygon using the function above\n",
    "dbscan_voronoi_intersection = combine_floods_within_voronoi(\n",
    "    voronoi_polygons = voronoi_polygons,  # Voronoi polygons GeoDataFrame\n",
    "    flood_polygons = dbscan_flood_areas,  # Flood polygons GeoDataFrame\n",
    "    voronoi_points = voronoi_centroid_points,  # Voronoi centroids GeoDataFrame\n",
    "    max_distance = 8_000  # Maximum distance (in km) for filtering based on proximity to Voronoi points\n",
    ")\n",
    "\n",
    "# Add the resulting dbscan_voronoi_intersection layer to a processing file (GeoPackage format)\n",
    "layer_name = \"dbscan_voronoi_intersection\"\n",
    "add_Layer_to_File(dbscan_voronoi_intersection, processing_file, layer_name, \"GPKG\")\n",
    "\n",
    "# Calculate initial and final intersections\n",
    "flood_protection_coastline, flood_protection_areas = linestring_intersect_polygons(\n",
    "    default_inland_distance=0.5,  # Buffer radius in kilometers\n",
    "    coast=coastline_edge_layer,  # Coastline GeoDataFrame\n",
    "    polygons=voronoi_polygons,  # Voronoi polygons GeoDataFrame\n",
    "    flood_polygons=dbscan_voronoi_intersection,  # Flood polygons GeoDataFrame from previous step\n",
    ")\n",
    "\n",
    "# Save the raw intersections to the processing file\n",
    "add_Layer_to_File(flood_protection_coastline, processing_file, \"flood_protection_coast\", \"GPKG\")\n",
    "\n",
    "# Save the final flood protection areas to the processing file\n",
    "add_Layer_to_File(flood_protection_areas, processing_file, \"flood_protection_areas\", \"GPKG\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_MultiPolygons(gdf):\n",
    "    \"\"\"\n",
    "    This function processes a GeoDataFrame (gdf) containing geometries of type Polygon or MultiPolygon.\n",
    "    It selects the largest Polygon from each MultiPolygon based on area, and leaves individual Polygons unchanged.\n",
    "    \n",
    "    Args:\n",
    "        gdf (GeoDataFrame): A GeoDataFrame containing geometries that can be either Polygons or MultiPolygons.\n",
    "        \n",
    "    Returns:\n",
    "        GeoDataFrame: A modified GeoDataFrame with only the largest Polygon selected from each MultiPolygon.\n",
    "    \"\"\"\n",
    "    \n",
    "    def largest_polygon(geom):\n",
    "        \"\"\"\n",
    "        This helper function determines the largest polygon in a MultiPolygon, \n",
    "        or returns the Polygon as-is if it is not a MultiPolygon.\n",
    "        \n",
    "        Args:\n",
    "            geom: The geometry object (either a Polygon or MultiPolygon).\n",
    "        \n",
    "        Returns:\n",
    "            Polygon: The largest Polygon from a MultiPolygon, or the Polygon as-is if it is already a single Polygon.\n",
    "        \"\"\"\n",
    "        if isinstance(geom, MultiPolygon):\n",
    "            # If the geometry is a MultiPolygon, find and return the largest Polygon based on area\n",
    "            return max(geom.geoms, key=lambda poly: poly.area)  # Access the 'geoms' attribute of the MultiPolygon\n",
    "        elif isinstance(geom, Polygon):\n",
    "            # If the geometry is already a Polygon, return it as it is\n",
    "            return geom\n",
    "        else:\n",
    "            # If the geometry is neither a Polygon nor a MultiPolygon, return it unchanged or handle it differently\n",
    "            return geom\n",
    "\n",
    "    # Apply the 'largest_polygon' function to each geometry in the GeoDataFrame\n",
    "    # This updates the 'geometry' column in the GeoDataFrame with the cleaned geometries\n",
    "    gdf['geometry'] = gdf['geometry'].apply(largest_polygon)\n",
    "\n",
    "    # Return the modified GeoDataFrame with cleaned geometries\n",
    "    return gdf\n",
    "\n",
    "\n",
    "# Apply the 'clean_MultiPolygons' function to the GeoDataFrame containing flood protection areas\n",
    "final_coastal_protection_area = clean_MultiPolygons(\n",
    "    gdf = flood_protection_areas  # GeoDataFrame containing flood protection polygons or multipolygons\n",
    ")\n",
    "\n",
    "# Process the cleaned GeoDataFrame by adding the maximum zonal statistics from the raster data\n",
    "final_coastal_protection_area = add_max_zonal_stats(\n",
    "    geopkg = final_coastal_protection_area,  # The cleaned GeoDataFrame\n",
    "    raster_path = raster_file,  # Path to the raster file containing flood height data\n",
    "    new_column_name= \"max_flood_height\"  # New column to store the maximum flood height value\n",
    ")\n",
    "\n",
    "# Clip the cleaned and processed GeoDataFrame to the boundaries of a specific polygon (e.g., a country or region)\n",
    "# final_coastal_protection_area = gpd.clip(final_coastal_protection_area, jamaica_polygon_revised)\n",
    "\n",
    "\n",
    "# Define the layer name to be used for storing the final protection area in a GeoPackage\n",
    "layer_name = \"final_protection_area\"\n",
    "\n",
    "# Add the final processed GeoDataFrame to the processing GeoPackage\n",
    "add_Layer_to_File(final_coastal_protection_area, processing_file, layer_name, \"GPKG\")\n",
    "\n",
    "# Also save the final protection area to the output file in GeoPackage format\n",
    "add_Layer_to_File(final_coastal_protection_area, output_file, layer_name, \"GPKG\")\n",
    "\n",
    "final_coastal_protection_coastline = flood_protection_coastline\n",
    "layer_name = \"final_coastal_protection_edges\"\n",
    "add_Layer_to_File(final_coastal_protection_coastline, output_file, layer_name, \"GPKG\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
